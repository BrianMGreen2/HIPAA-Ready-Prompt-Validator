"""
llm_prompt_validation.py

Companion Python functions for LLM Prompt Security & Accuracy Validation in Healthcare Context.
Each function generates a specific validation prompt with user-supplied data.
"""

def prompt_phi_scrubbing(user_input):
    return (
        "Review the following submission for any personally identifiable or protected health information. "
        "If found, flag its presence and suggest redactions.\n"
        f"Input: {user_input}"
    )

def prompt_input_sanitization(structured_fields):
    return (
        "Validate the following input fields for forbidden characters, excessive length, or suspicious patterns "
        "(e.g., code, URLs, special characters). List any fields that do not comply.\n"
        f"Input: {structured_fields}"
    )

def prompt_output_redaction(llm_output):
    return (
        "Analyze the model-generated output below. Does it contain any names, full dates of birth, addresses, "
        "or other direct identifiers? If so, recommend masking or redacting those items.\n"
        f"Output: {llm_output}"
    )

def prompt_access_controls(user_role):
    return (
        "Given this prompt and user role ({user_role}), check if the requested action should be permitted based on "
        "least privilege and HIPAA access standards. Flag any access violations."
    ).format(user_role=user_role)

def prompt_audit_logging(log_entry):
    return (
        "Audit this transaction: confirm that only pseudonymized values are in logs and no PHI is present. "
        "Identify any privacy risks in the following log entry:\n"
        f"Log: {log_entry}"
    )

def prompt_regulatory_compliance():
    return (
        "Review this prompt workflow for HIPAA compliance: Are data transmission, storage, and access protocols "
        "explicitly described and followed? Summarize gaps, if any."
    )

def prompt_test_case_suite(scenario, prompt_text):
    return (
        "Given the following patient scenario and test prompt, generate the model response. Then, evaluate whether "
        "the output is clinically accurate and relevant. Flag any errors or omissions.\n"
        f"Scenario: {scenario}\nPrompt: {prompt_text}"
    )

def prompt_automated_regression(current_output, approved_output):
    return (
        "Compare current output for this test scenario with the previously approved output. Are there new errors, "
        "loss of clinical detail, or formatting issues? Describe differences numerically and in plain language.\n"
        f"Current Output: {current_output}\nApproved Output: {approved_output}"
    )

def prompt_peer_review(prompt_and_output):
    return (
        "As a clinical reviewer, does this prompt (and output) use the correct medical terminology, contain no "
        "ambiguity, and follow unit standards? Provide a short peer review and approve/reject.\n"
        f"Input: {prompt_and_output}"
    )

def prompt_fact_checking(llm_output, reference_guidelines=None):
    message = (
        "Fact-check this LLM-generated summary against authoritative guidelines"
    )
    if reference_guidelines:
        message += f" (e.g., {reference_guidelines})"
    message += ". Are all medical assertions supported? List discrepancies or confirm if fully accurate.\n"
    message += f"Output: {llm_output}"
    return message

def prompt_bias_safety_check(llm_output):
    return (
        "Review this output for signs of inappropriate bias (e.g., race, gender, age) or non-standard, unsafe "
        "recommendations. Report any bias or unsafe guidance.\n"
        f"Output: {llm_output}"
    )

def prompt_drift_detection(current_outputs, historical_outputs, time_window="last month"):
    return (
        f"Compare this weekâ€™s outputs for the same prompt against {time_window}'s validated outputs. Is there any drift in "
        "quality, terminology, or completeness? Summarize key changes and flag if further review is needed.\n"
        f"Current Outputs: {current_outputs}\nHistorical Outputs: {historical_outputs}"
    )

# Example usage/test
if __name__ == "__main__":
    # Example data
    example_user_input = "Patient John Doe, DOB: 01/15/1970, ID: 1234, reports BP 145/90."
    example_structured_fields = {"Name": "John Doe", "BP": "145/90", "Notes": "N/A"}
    example_llm_output = "Patient John Doe had an abnormal BP reading of 145/90."
    example_log_entry = "User123 accessed record for patient Pt98765 at 10:32am."
    example_current_output = "BP: 145/90\nRecommendation: Monitor."
    example_approved_output = "BP: 145/90\nRecommendation: Follow-up with PCP."
    example_scenario = "65-year-old male with hypertension, reported BP readings."
    example_prompt_text = "Summarize abnormal BPs for the provider."
    
    print(prompt_phi_scrubbing(example_user_input))
    print(prompt_input_sanitization(example_structured_fields))
    print(prompt_output_redaction(example_llm_output))
    print(prompt_access_controls("nurse"))
    print(prompt_audit_logging(example_log_entry))
    print(prompt_regulatory_compliance())
    print(prompt_test_case_suite(example_scenario, example_prompt_text))
    print(prompt_automated_regression(example_current_output, example_approved_output))
    print(prompt_peer_review(example_prompt_text + "\n" + example_current_output))
    print(prompt_fact_checking(example_llm_output, reference_guidelines="CDC, Institutional Protocols"))
    print(prompt_bias_safety_check(example_llm_output))
    print(prompt_drift_detection([example_current_output], [example_approved_output]))
